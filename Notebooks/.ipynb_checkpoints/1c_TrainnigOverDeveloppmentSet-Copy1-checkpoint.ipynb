{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a911ba53-f148-436a-a84b-a107b118cb5e",
   "metadata": {},
   "source": [
    "# Train the network on developpment set\n",
    "\n",
    "\n",
    "NB : faire un test sur le set d'evaluation avec quelques fichiers test avec des calculs de Precision, Recall, quelques spectros, les labels \n",
    "\n",
    "## Import functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e4e47-d0ea-4958-a22d-da09b20076c7",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">*JUST RUN CELL*</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239dbd34-388b-4dfe-94a7-02fb4184eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "#os.chdir(os.path.join(\"/home/datawork-osmose/\",'osmoseNotebooks_v0','source'))\n",
    "\n",
    "with open('path_codes.txt') as f:\n",
    "    codes_path = f.readlines()[0]\n",
    "os.chdir(os.path.join(codes_path))\n",
    "\n",
    "from train_network import TrainNetwork_main, plot_examples_from_test\n",
    "from launcher_datasetScale import list_datasets\n",
    "from check_files_in_ai_folders import check_available_ai_tasks_bm, check_available_ai_datasplit, check_available_formats, check_available_ai_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431855bb-1d21-48a5-8756-f29c59125a6f",
   "metadata": {},
   "source": [
    "## Selection of task, benchmark and datasplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ec9af4-f81b-49be-a3f1-501befecaa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "TaskTEST1/\n",
      "    BMTEST1_1/\n",
      "             Dataset used :  ['MPSU_ForestouHuella' 'MPSU_ForestouHuella_copy']\n",
      "Task_Glider_PBW_AnnotatorAnalysis/\n",
      "    BM_merged15/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_EXP/\n",
      "             Dataset used :  ['Glider']\n",
      "    Comp_BM_curves/\n",
      "    BM_merged6/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_2/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_3/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_4/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_5/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_6/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_7/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_8/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged8/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged10/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged12/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged14/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_9/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_10/\n",
      "             Dataset used :  ['Glider']\n",
      "    WeakLabelling_AllAnnotation/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Nassau/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Shanghai/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Civitavecchia/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Galveston/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Venice/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Fukuoka/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Naples/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Bridgetown/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Valletta/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Rostock/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Dubrovnik/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Mahahual/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Ensenada/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Tunis/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Funchal/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Tallinn/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Malaga/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Helsinki/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Valencia/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_FULL_EVAL/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_FULL_EVAL_long/\n",
      "             Dataset used :  ['Glider']\n",
      "Task_Test_Apocado_whistles/\n",
      "    BM_1/\n",
      "             Dataset used :  ['APOCADO_IROISE_C2D1_07072022']\n",
      "Task_Det_PBW/\n",
      "    TEST1/\n",
      "             Dataset used :  ['Dataset2015_AUS']\n",
      "    TRAIN_Glider/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_FULL_EVAL/\n",
      "             Dataset used :  ['Glider']\n",
      "    TRAIN_Glider_long/\n",
      "             Dataset used :  ['Glider']\n"
     ]
    }
   ],
   "source": [
    "check_available_ai_tasks_bm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae4f7ff0-218c-4f8f-8ea1-0eb67f72afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Task_ID = 'Task_Det_PBW'\n",
    "BM_Name = 'TRAIN_Glider_long'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff53197-4138-4010-a3bd-6bc4a6572b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasplits available in this task and this benchmark : \n",
      "info_datasplit/\n",
      "    split_for_eval_1/\n"
     ]
    }
   ],
   "source": [
    "check_available_ai_datasplit(Task_ID, BM_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08587151-5ae7-4c02-9609-4b46492ba3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "SplitName = 'split_for_eval_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb4661-7cbd-4108-b5c5-026dba243ff1",
   "metadata": {},
   "source": [
    "## Check model \n",
    "\n",
    "Choose the name of your new model. If the name already exist, it will be overwrited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69830632-584e-4fb6-a077-13f02d7c43d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models available in this task and this benchmark : \n"
     ]
    }
   ],
   "source": [
    "check_available_ai_model(Task_ID, BM_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc89058-5350-4ce8-a01a-b7f939860629",
   "metadata": {},
   "outputs": [],
   "source": [
    "Version_name = 'm1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2153ee-4c18-4547-b9c4-58d51741e5e9",
   "metadata": {},
   "source": [
    "## Check all spectrograms already available \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64357c2f-a618-4327-99c8-6a108f7b3bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________\n",
      "Dataset :  Glider\n",
      "Available Spectrogram Format (nfft_windowsize_overlap) :\n",
      "     50_500\n",
      "         499_198_50\n",
      "     50_250\n",
      "         512_512_92\n"
     ]
    }
   ],
   "source": [
    "check_available_formats(Task_ID, BM_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab06be6-d9a0-4314-9c34-bec136745922",
   "metadata": {},
   "source": [
    "\n",
    "Enter the spectrogram format in the corresponding parameters.\n",
    "As list, to match with the list of dataset used in the benchmark (checked above).\n",
    "\n",
    "Example with 2 dataset : \n",
    "- ``nfft`` = [1024, 4096] \n",
    "- ``window_size`` = [1024, 1024] \n",
    "- ``overlap`` = [80, 50]\n",
    "\n",
    "\n",
    "It is also possible to use only .wav file as input, new spectrograms will be computed iteratively.\n",
    "In this case, please fill also some other parameters for the computation of the new spectrograms : \n",
    "\n",
    "Nb : The wavefiles are normalized one by one using \"data = (data - np.mean(data)) / np.std(data)\". The normalization from instruments values to get absolute pressure level will be add on the next version.      \n",
    "\n",
    "\n",
    "- ``dynamic_min`` = [-40, -40]\n",
    "- ``dynamic_max`` = [40, 40]\n",
    "- ``scaling`` = ['spectrum', 'spectrum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97707b8e-9f1b-411f-8ce9-1f6b02f9187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_format = 'audio' # 'audio'\n",
    "# 'spectrogram' : use already computed spectrogram\n",
    "# 'audio' : use .wav file, spectrogram will be computed iteratively\n",
    "\n",
    "nfft = [512]\n",
    "window_size = [512]\n",
    "overlap = [92]\n",
    "\n",
    "\n",
    "#Useless if input_data_format = 'spectrogram'\n",
    "dynamic_min = [-20]\n",
    "dynamic_max = [20]\n",
    "scaling = ['spectrum'] # 'spectrum', 'density'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088aa848-8099-4ad8-a6a7-d82476a70649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c3d22fa-9f9a-404a-b8b0-bbe0d344743a",
   "metadata": {},
   "source": [
    "### Set model parameters\n",
    "\n",
    "#### <span style=\"color:green\">*LIST OF PARAMETERS*</span>\n",
    "\n",
    "- `` ModelName `` : Architecture of the model. Please, select one in this list : (for more details, check : https://pytorch.org/vision/main/models.html)\n",
    "    - 'CNN3_FC1'\n",
    "    - 'CNN3_FC3'\n",
    "    - 'resnet18'\n",
    "    - 'resnet50'\n",
    "    - 'resnet101'\n",
    "    - 'vgg11'\n",
    "    - 'vgg11_bn'\n",
    "    - 'vgg13'\n",
    "    - 'vgg13_bn'\n",
    "    - 'vgg19'\n",
    "    - 'vgg19_bn'\n",
    "    - 'alexnet'\n",
    "   \n",
    "    \n",
    "- `` use_pretrained `` : (for more details, check : https://pytorch.org/vision/main/models.html) (By default : use_pretrained = True)\n",
    "    - True : if you want to used already pretrained network one reference image dataset and just finetun the last layer \n",
    "    - False : if you want to train your network from random weights and adjust all layers\n",
    "\n",
    "\n",
    "       \n",
    "- `` TrainsetRatio `` : ratio between 0 and 1 of all the developpement set that will be used for the training (the oser part is for testing). (if None : TrainSetRatio = 0.9)\n",
    "\n",
    "- `` batch_size `` : Number of spectrograms per batch (if None : batch_size = 10)\n",
    "- `` learning_rate `` : step size at each iteration while moving toward a minimum of a loss function (if None : learning_rate = 1e-3)\n",
    "- `` num_epochs `` : Number of iteration over all developpement set (if None : num_epochs = 10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b5ff7b2-1114-49a7-b0e6-560b6d8916f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelName = 'CNN3_FC3'\n",
    "use_pretrained = True\n",
    "\n",
    "TrainsetRatio = 0.8\n",
    "batch_size = 15\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 30\n",
    "shuffle = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d55d279e-15c9-4b18-b296-52a752f3b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'TrainsetRatio':TrainsetRatio, 'batch_size':batch_size, 'learning_rate':learning_rate, 'num_epochs':num_epochs, 'shuffle':shuffle, 'use_pretrained':use_pretrained, 'input_data_format':input_data_format, 'nfft':nfft, 'window_size':window_size, 'overlap':overlap, 'dynamic_min':dynamic_min, 'dynamic_max':dynamic_max, 'scaling':scaling}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e00eef9-fa5e-4db1-9c81-624b71c70fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL INFORMATION : \n",
      " \n",
      "CNN3_FC3(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (lin1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (lin2): Linear(in_features=128, out_features=16, bias=True)\n",
      "  (lin3): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (dout): Dropout(p=0.25, inplace=False)\n",
      ")\n",
      "Params to learn:\n",
      "\t conv1.0.weight\n",
      "\t conv1.0.bias\n",
      "\t conv1.1.weight\n",
      "\t conv1.1.bias\n",
      "\t conv2.0.weight\n",
      "\t conv2.0.bias\n",
      "\t conv2.1.weight\n",
      "\t conv2.1.bias\n",
      "\t conv3.0.weight\n",
      "\t conv3.0.bias\n",
      "\t conv3.1.weight\n",
      "\t conv3.1.bias\n",
      "\t lin1.weight\n",
      "\t lin1.bias\n",
      "\t lin2.weight\n",
      "\t lin2.bias\n",
      "\t lin3.weight\n",
      "\t lin3.bias\n",
      " \n",
      "TRAINNING : \n",
      "Epoch TRAIN [1/30]  -- Loss = 0.6769153483686454  --  iteration [1429/1429]  \n",
      "Epoch TEST [1/30]  -- Loss = 0.612906165483619  --  iteration [357/357]  \n",
      "Epoch TRAIN [2/30]  -- Loss = 0.5622277845236869  --  iteration [1429/1429]  \n",
      "Epoch TEST [2/30]  -- Loss = 0.4078675865375695  --  iteration [357/357]  \n",
      "Epoch TRAIN [3/30]  -- Loss = 0.456570906591966  --  iteration [1429/1429]9]  \n",
      "Epoch TEST [3/30]  -- Loss = 0.330336695208269  --  iteration [357/357]7]  \n",
      "Epoch TRAIN [4/30]  -- Loss = 0.41742852409458225  --  iteration [1429/1429]  \n",
      "Epoch TEST [4/30]  -- Loss = 0.3048605696410358  --  iteration [357/357]]  \n",
      "Epoch TRAIN [5/30]  -- Loss = 0.39176753855214574  --  iteration [1429/1429]  \n",
      "Epoch TEST [5/30]  -- Loss = 0.2871465995824304  --  iteration [357/357]]  \n",
      "Epoch TRAIN [6/30]  -- Loss = 0.37582634807029247  --  iteration [1429/1429]  \n",
      "Epoch TEST [6/30]  -- Loss = 0.2750382772555538  --  iteration [357/357]]  \n",
      "Epoch TRAIN [7/30]  -- Loss = 0.36038219495992546  --  iteration [1429/1429]  \n",
      "Epoch TEST [7/30]  -- Loss = 0.26866828938241766  --  iteration [357/357]  \n",
      "Epoch TRAIN [8/30]  -- Loss = 0.34803583670050386  --  iteration [1429/1429]  \n",
      "Epoch TEST [8/30]  -- Loss = 0.25666756714049843  --  iteration [357/357]  \n",
      "Epoch TRAIN [9/30]  -- Loss = 0.3359384560909323  --  iteration [1429/1429]]  \n",
      "Epoch TEST [9/30]  -- Loss = 0.2464413156195515  --  iteration [357/357]]  \n",
      "Epoch TRAIN [10/30]  -- Loss = 0.3248007008199762  --  iteration [1429/1429]]  \n",
      "Epoch TEST [10/30]  -- Loss = 0.24439038342538  --  iteration [357/357]57]  \n",
      "Epoch TRAIN [11/30]  -- Loss = 0.31481664125176057  --  iteration [1429/1429]  \n",
      "Epoch TEST [11/30]  -- Loss = 0.24060916468626312  --  iteration [357/357]  \n",
      "Epoch TRAIN [12/30]  -- Loss = 0.3063420301721678  --  iteration [1429/1429]  \n",
      "Epoch TEST [12/30]  -- Loss = 0.23631332012242964  --  iteration [357/357]  \n",
      "Epoch TRAIN [13/30]  -- Loss = 0.2963478045448129  --  iteration [1429/1429]]  \n",
      "Epoch TEST [13/30]  -- Loss = 0.23266154805234834  --  iteration [357/357]  \n",
      "Epoch TRAIN [14/30]  -- Loss = 0.2866547287886922  --  iteration [1429/1429]]  \n",
      "Epoch TEST [14/30]  -- Loss = 0.23481066833858064  --  iteration [357/357]  \n",
      "Epoch TRAIN [15/30]  -- Loss = 0.2752111496196853  --  iteration [90/1429]]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_ft, test_loader, LabelsList \u001b[38;5;241m=\u001b[39m \u001b[43mTrainNetwork_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTask_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBM_Name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSplitName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVersion_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModelName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\PhD\\CodesDatarmor\\OSmOSEcodesV2\\Functions\\train_network.py:277\u001b[0m, in \u001b[0;36mTrainNetwork_main\u001b[1;34m(Task_ID, BM_Name, SplitName, Version_name, ModelName, parameters)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAINNING : \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m#Launch the training \u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m loss_tab_train, loss_tab_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVersion_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModelName\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDONE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m#%% Save Model \u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m#save model\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\PhD\\CodesDatarmor\\OSmOSEcodesV2\\Functions\\train_network.py:64\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(device, model_ft, optimizer, num_epochs, train_loader, test_loader, weight, model_path, Version_name, ModelName)\u001b[0m\n\u001b[0;32m     62\u001b[0m epoch_p \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m#Loop For over train set\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m#load data and label, send them to device\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     67\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1186\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1186\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1189\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1142\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1142\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1144\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:990\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 990\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m    992\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    993\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m    994\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m    995\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft, test_loader, LabelsList = TrainNetwork_main(Task_ID, BM_Name, SplitName, Version_name, ModelName, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9910761-480d-4cea-a6d8-28e31fdfcfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import TrainNetwork_main, plot_examples_from_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1501b47-c66b-4752-a23b-16643b04c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples_from_test(model_ft, test_loader, LabelsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b189a-ad43-462f-b0b9-8f79c1a3b9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
