{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a911ba53-f148-436a-a84b-a107b118cb5e",
   "metadata": {},
   "source": [
    "# Train the network on developpment set\n",
    "\n",
    "\n",
    "NB : faire un test sur le set d'evaluation avec quelques fichiers test avec des calculs de Precision, Recall, quelques spectros, les labels \n",
    "\n",
    "## Import functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e4e47-d0ea-4958-a22d-da09b20076c7",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">*JUST RUN CELL*</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239dbd34-388b-4dfe-94a7-02fb4184eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "#os.chdir(os.path.join(\"/home/datawork-osmose/\",'osmoseNotebooks_v0','source'))\n",
    "\n",
    "with open('path_codes.txt') as f:\n",
    "    codes_path = f.readlines()[0]\n",
    "os.chdir(os.path.join(codes_path))\n",
    "\n",
    "from train_network import TrainNetwork_main, plot_examples_from_test\n",
    "from launcher_datasetScale import list_datasets\n",
    "from check_files_in_ai_folders import check_available_ai_tasks_bm, check_available_ai_datasplit, check_available_formats, check_available_ai_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431855bb-1d21-48a5-8756-f29c59125a6f",
   "metadata": {},
   "source": [
    "## Selection of task, benchmark and datasplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ec9af4-f81b-49be-a3f1-501befecaa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "TaskTEST1/\n",
      "    BMTEST1_1/\n",
      "             Dataset used :  ['MPSU_ForestouHuella' 'MPSU_ForestouHuella_copy']\n",
      "Task_Glider_PBW_AnnotatorAnalysis/\n",
      "    BM_merged15/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_EXP/\n",
      "             Dataset used :  ['Glider']\n",
      "    Comp_BM_curves/\n",
      "    BM_merged6/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_2/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_3/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_4/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_5/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_6/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_7/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_8/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged8/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged10/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged12/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged14/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_9/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_merged2_10/\n",
      "             Dataset used :  ['Glider']\n",
      "    WeakLabelling_AllAnnotation/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Nassau/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Shanghai/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Civitavecchia/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Galveston/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Venice/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Fukuoka/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Naples/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Bridgetown/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Valletta/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Rostock/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Dubrovnik/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Mahahual/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Ensenada/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Tunis/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Funchal/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Tallinn/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Malaga/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Helsinki/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_AN_Valencia/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_FULL_EVAL/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_FULL_EVAL_long/\n",
      "             Dataset used :  ['Glider']\n",
      "Task_Test_Apocado_whistles/\n",
      "    BM_1/\n",
      "             Dataset used :  ['APOCADO_IROISE_C2D1_07072022']\n",
      "Task_Det_PBW/\n",
      "    TEST1/\n",
      "             Dataset used :  ['Dataset2015_AUS']\n",
      "    TRAIN_Glider/\n",
      "             Dataset used :  ['Glider']\n",
      "    BM_FULL_EVAL/\n",
      "             Dataset used :  ['Glider']\n",
      "    TRAIN_Glider_long/\n",
      "             Dataset used :  ['Glider']\n"
     ]
    }
   ],
   "source": [
    "check_available_ai_tasks_bm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae4f7ff0-218c-4f8f-8ea1-0eb67f72afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Task_ID = 'Task_Det_PBW'\n",
    "BM_Name = 'TRAIN_Glider'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ff53197-4138-4010-a3bd-6bc4a6572b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasplits available in this task and this benchmark : \n",
      "info_datasplit/\n",
      "    split_for_eval_1/\n"
     ]
    }
   ],
   "source": [
    "check_available_ai_datasplit(Task_ID, BM_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08587151-5ae7-4c02-9609-4b46492ba3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "SplitName = 'split_for_eval_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb4661-7cbd-4108-b5c5-026dba243ff1",
   "metadata": {},
   "source": [
    "## Check model \n",
    "\n",
    "Choose the name of your new model. If the name already exist, it will be overwrited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69830632-584e-4fb6-a077-13f02d7c43d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models available in this task and this benchmark : \n",
      "    m1/\n"
     ]
    }
   ],
   "source": [
    "check_available_ai_model(Task_ID, BM_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fc89058-5350-4ce8-a01a-b7f939860629",
   "metadata": {},
   "outputs": [],
   "source": [
    "Version_name = 'm3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2153ee-4c18-4547-b9c4-58d51741e5e9",
   "metadata": {},
   "source": [
    "## Check all spectrograms already available \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64357c2f-a618-4327-99c8-6a108f7b3bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________\n",
      "Dataset :  Glider\n",
      "Available Spectrogram Format (nfft_windowsize_overlap) :\n",
      "     50_500\n",
      "         499_198_50\n",
      "     50_250\n",
      "         512_512_92\n"
     ]
    }
   ],
   "source": [
    "check_available_formats(Task_ID, BM_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab06be6-d9a0-4314-9c34-bec136745922",
   "metadata": {},
   "source": [
    "\n",
    "Enter the spectrogram format in the corresponding parameters.\n",
    "As list, to match with the list of dataset used in the benchmark (checked above).\n",
    "\n",
    "Example with 2 dataset : \n",
    "- ``nfft`` = [1024, 4096] \n",
    "- ``window_size`` = [1024, 1024] \n",
    "- ``overlap`` = [80, 50]\n",
    "\n",
    "\n",
    "It is also possible to use only .wav file as input, new spectrograms will be computed iteratively.\n",
    "In this case, please fill also some other parameters for the computation of the new spectrograms : \n",
    "\n",
    "Nb : The wavefiles are normalized one by one using \"data = (data - np.mean(data)) / np.std(data)\". The normalization from instruments values to get absolute pressure level will be add on the next version.      \n",
    "\n",
    "\n",
    "- ``dynamic_min`` = [-40, -40]\n",
    "- ``dynamic_max`` = [40, 40]\n",
    "- ``scaling`` = ['spectrum', 'spectrum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97707b8e-9f1b-411f-8ce9-1f6b02f9187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_format = 'audio' # 'audio'\n",
    "# 'spectrogram' : use already computed spectrogram\n",
    "# 'audio' : use .wav file, spectrogram will be computed iteratively\n",
    "\n",
    "nfft = [512]\n",
    "window_size = [512]\n",
    "overlap = [92]\n",
    "\n",
    "\n",
    "#Useless if input_data_format = 'spectrogram'\n",
    "dynamic_min = [-20]\n",
    "dynamic_max = [20]\n",
    "scaling = ['spectrum'] # 'spectrum', 'density'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088aa848-8099-4ad8-a6a7-d82476a70649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c3d22fa-9f9a-404a-b8b0-bbe0d344743a",
   "metadata": {},
   "source": [
    "### Set model parameters\n",
    "\n",
    "#### <span style=\"color:green\">*LIST OF PARAMETERS*</span>\n",
    "\n",
    "- `` ModelName `` : Architecture of the model. Please, select one in this list : (for more details, check : https://pytorch.org/vision/main/models.html)\n",
    "    - 'CNN3_FC1'\n",
    "    - 'CNN3_FC3'\n",
    "    - 'resnet18'\n",
    "    - 'resnet50'\n",
    "    - 'resnet101'\n",
    "    - 'vgg11'\n",
    "    - 'vgg11_bn'\n",
    "    - 'vgg13'\n",
    "    - 'vgg13_bn'\n",
    "    - 'vgg19'\n",
    "    - 'vgg19_bn'\n",
    "    - 'alexnet'\n",
    "   \n",
    "    \n",
    "- `` use_pretrained `` : (for more details, check : https://pytorch.org/vision/main/models.html) (By default : use_pretrained = True)\n",
    "    - True : if you want to used already pretrained network one reference image dataset and just finetun the last layer \n",
    "    - False : if you want to train your network from random weights and adjust all layers\n",
    "\n",
    "\n",
    "       \n",
    "- `` TrainsetRatio `` : ratio between 0 and 1 of all the developpement set that will be used for the training (the oser part is for testing). (if None : TrainSetRatio = 0.9)\n",
    "\n",
    "- `` batch_size `` : Number of spectrograms per batch (if None : batch_size = 10)\n",
    "- `` learning_rate `` : step size at each iteration while moving toward a minimum of a loss function (if None : learning_rate = 1e-3)\n",
    "- `` num_epochs `` : Number of iteration over all developpement set (if None : num_epochs = 10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b5ff7b2-1114-49a7-b0e6-560b6d8916f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelName = 'CNN3_FC1'\n",
    "use_pretrained = True\n",
    "\n",
    "TrainsetRatio = 0.8\n",
    "batch_size = 15\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 30\n",
    "shuffle = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d55d279e-15c9-4b18-b296-52a752f3b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'TrainsetRatio':TrainsetRatio, 'batch_size':batch_size, 'learning_rate':learning_rate, 'num_epochs':num_epochs, 'shuffle':shuffle, 'use_pretrained':use_pretrained, 'input_data_format':input_data_format, 'nfft':nfft, 'window_size':window_size, 'overlap':overlap, 'dynamic_min':dynamic_min, 'dynamic_max':dynamic_max, 'scaling':scaling}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00eef9-fa5e-4db1-9c81-624b71c70fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL INFORMATION : \n",
      " \n",
      "CNN3_FC1(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (lin1): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (dout): Dropout(p=0.25, inplace=False)\n",
      ")\n",
      "Params to learn:\n",
      "\t conv1.0.weight\n",
      "\t conv1.0.bias\n",
      "\t conv1.1.weight\n",
      "\t conv1.1.bias\n",
      "\t conv2.0.weight\n",
      "\t conv2.0.bias\n",
      "\t conv2.1.weight\n",
      "\t conv2.1.bias\n",
      "\t conv3.0.weight\n",
      "\t conv3.0.bias\n",
      "\t conv3.1.weight\n",
      "\t conv3.1.bias\n",
      "\t lin1.weight\n",
      "\t lin1.bias\n",
      " \n",
      "TRAINNING : \n",
      "Epoch TRAIN [1/30]  -- Loss = 0.5991334952075136  --  iteration [1429/1429]  \n",
      "Epoch TEST [1/30]  -- Loss = 0.42956612415674356  --  iteration [357/357]  \n",
      "Epoch TRAIN [2/30]  -- Loss = 0.5025474111451145  --  iteration [1429/1429]  \n",
      "Epoch TEST [2/30]  -- Loss = 0.41242319487390067  --  iteration [357/357]  \n",
      "Epoch TRAIN [3/30]  -- Loss = 0.46926200692264913  --  iteration [1429/1429]  \n",
      "Epoch TEST [3/30]  -- Loss = 0.3905607593827555  --  iteration [357/357]]  \n",
      "Epoch TRAIN [4/30]  -- Loss = 0.453004243471474  --  iteration [1429/1429]9]  \n",
      "Epoch TEST [4/30]  -- Loss = 0.39263556732767435  --  iteration [357/357]  \n",
      "Epoch TRAIN [5/30]  -- Loss = 0.447300790133886  --  iteration [1280/1429]9]"
     ]
    }
   ],
   "source": [
    "model_ft, test_loader, LabelsList = TrainNetwork_main(Task_ID, BM_Name, SplitName, Version_name, ModelName, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9910761-480d-4cea-a6d8-28e31fdfcfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import TrainNetwork_main, plot_examples_from_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1501b47-c66b-4752-a23b-16643b04c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples_from_test(model_ft, test_loader, LabelsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b189a-ad43-462f-b0b9-8f79c1a3b9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
